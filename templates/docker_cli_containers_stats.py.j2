#!/usr/bin/env python

import argparse
import docker
import hashlib
import json
import os
import re
import sys
import time

# Return dict
content=None

# python 3
if sys.version_info[0] == 3:
    string_types = str,
else:
    string_types = basestring,

# Global variables
# number of second to cache a query result
local_query_cache_life = 55
# the path to the cache directory
local_query_cache_dir_path = '/tmp/zbx_dck_csqc_{}'.format(hashlib.md5(os.getenv('USER').encode()).hexdigest())

# the base path of the sys fs tree
sysfs_base_path = '/sys/fs/cgroup'
# list of cgroup tree to discover
sysfs_metrics_config = {
    'cpu': {
        'cpu.stat.*': {'single': True, 'type': int},
        'cpu.cfs_period_us': {'single': True, 'type': int},
        'cpu.cfs_quota_us': {'single': True, 'type': int},
        'cpu.rt_period_us': {'single': True, 'type': int},
        'cpu.rt_runtime_us': {'single': True, 'type': int},
        'cpuacct.stat.*': {'single': True, 'type': int},
        'cpuacct.usage': {'single': True, 'type': int},
        'cpuacct.usage_percpu': {'single': True, 'indexed_colums': True},
    },
    'cpuacct': {
        'cpu.stat.*': {'single': True, 'type': int},
        'cpu.cfs_period_us': {'single': True, 'type': int},
        'cpu.cfs_quota_us': {'single': True, 'type': int},
        'cpu.rt_period_us': {'single': True, 'type': int},
        'cpu.rt_runtime_us': {'single': True, 'type': int},
        'cpuacct.stat.*': {'single': True, 'type': int},
        'cpuacct.usage': {'single': True, 'type': int},
        'cpuacct.usage_percpu': {'single': True, 'indexed_colums': True},
    },
    'memory': {
        'memory.usage_in_bytes': {'single': True, 'type': int},
        'memory.max_usage_in_bytes': {'single': True, 'type': int},
        'memory.limit_in_bytes': {'single': True, 'type': int},
        'memory.soft_limit_in_bytes': {'single': True, 'type': int},
        'memory.memsw.*': {'single': True, 'type': int},
        'memory.kmem.usage_in_bytes': {'single': True, 'type': int},
        'memory.kmem.max_usage_in_bytes': {'single': True, 'type': int},
        'memory.kmem.limit_in_bytes': {'single': True, 'type': int},
        'memory.kmem.failcnt': {'single': True, 'type': int},
        'memory.kmem.tcp.*': {'single': True, 'type': int},
        'memory.oom_control_count': {'single': True, 'type': int},
        'memory.swappiness': {'single': True, 'type': int},
        'memory.use_hierarchy': {'single': True, 'type': int},
        'memory.failcnt': {'single': True, 'type': int},
        'memory.stat.*': {'single': True, 'type': int},
        'memory.oom_control.*': {'single': True, 'type': int},
    },
    'blkio': {},
}
# This regular expression matchs all named metrics in sysfs files
re_named_metric = re.compile('(?P<name>[a-zA-Z][a-zA-Z0-9_-]*)\s*(?P<metric>[0-9.]+)')

# Docker cli configuration
os.environ['DOCKER_HOST']='{{ docker_server__clients_host }}'


def LockException(BaseException):
    """Simple exception to handle locking related errors
    """
    pass

def acquireLock(lockfile, max_tries=14):
    """Try to acquire lock with a lockfile

    @param str : the path to the lockfile
    @param int : the maximal number of lock tries
    @raise LockException on lock fail
    """
    tries = 0
    # wait if lock is acquired
    while os.path.isfile(lockfile):
        tries += 1
        if tries > max_tries:
            raise LockException('unable to acquire lock')
        time.sleep(0.1)
    with open(lockfile, 'w') as f:
        pass
    return True

def releaseLock(lockfile):
    """Remove the lockfile

    @param str : the path to the lockfile
    """
    if os.path.isfile(lockfile):
        os.unlink(lockfile)
    return True

def getContainersStatsFromApi(container_name, cache=True):
    local_query_cache_path = os.path.join(local_query_cache_dir_path, container_name)
    local_query_cache_lock_path = local_query_cache_path+'.lock'

    # generate cache
    if (not cache or
        not os.path.isfile(local_query_cache_path) or
        time.time() - os.stat(local_query_cache_path).st_mtime > local_query_cache_life):
        try:
            client = docker.from_env()
            if hasattr(client.containers, 'get'):
                stats = client.containers.get(container_name).stats(stream=False)
            else:
                stats = client.stats(container_name, stream=False)
        except docker.errors.NotFound:
            return None

        if not cache:
            return stats
        try:
            acquireLock(local_query_cache_lock_path)
            with open(local_query_cache_path, 'w') as f:
                json.dump(stats, f)
        except Exception as e:
            raise e
        finally:
            releaseLock(local_query_cache_lock_path)
    # use cache
    else:
        try:
            acquireLock(local_query_cache_lock_path)
            with open(local_query_cache_path, 'r') as f:
                stats = json.load(f)
        except Exception as e:
            raise e
        finally:
            releaseLock(local_query_cache_lock_path)
    return stats

def treatMetric(cgroup, metric, value, single=False):
    # use the configured type to store metric
    metric_config = None

    metric_wildcard = '.'.join(metric.split('.')[:-1] + ['*'])
    if metric in sysfs_metrics_config[cgroup]:
        metric_config = sysfs_metrics_config[cgroup][metric]
    elif metric_wildcard in sysfs_metrics_config[cgroup]:
        metric_config = sysfs_metrics_config[cgroup][metric_wildcard]

    if metric_config:
        # treat the content as a single value
        if 'single' in metric_config and metric_config['single'] or single:
            if isinstance(value, (list, dict)):
                if len(value) == 1:
                    value = value[0]
                else:
                    return '## ERROR {} non singlable'.format(str(value))
        if 'type' in metric_config:
            try:
                return metric_config['type'](value)
            except ValueError as e:
                return '## ERROR ' + str(e)
        if 'indexed_colums' in metric_config:
            result = dict()
            for v in value.split():
                result[len(result)] = v
            return result
        return value

    # test for metric format
    # Named metrics are in format : "NAME VALUE"
    named_metric = None
    if not isinstance(value, (list, dict)):
        value = [value]
    named_metric = re_named_metric.match(value[0])
    if named_metric:
        result = dict()
        for v in value:
            named_metric = re_named_metric.match(v)
            metric_name = named_metric.group('name')
            metric_value = named_metric.group('metric')
            if named_metric:
                result[metric_name] = treatMetric(cgroup, metric + '.' + metric_name, metric_value, single)
            else:
                result[metric_name] = '## ERROR format'
        return result
    # default behaviour is to put all items in a list
    return list(value)

def getContainersStatsFromSysFs(container_id, cache=True):
    local_query_cache_path = os.path.join(local_query_cache_dir_path, container_id)
    local_query_cache_lock_path = local_query_cache_path+'.lock'

    # generate cache
    if (not cache or
        not os.path.isfile(local_query_cache_path) or
        time.time() - os.stat(local_query_cache_path).st_mtime > local_query_cache_life):

        container_metrics = dict()
        for cgroup in sysfs_metrics_config:
            cgroup_path = os.path.join(sysfs_base_path, cgroup, 'docker', container_id)
            if not os.path.isdir(cgroup_path):
                continue

            # read all metrics from files
            container_metrics[cgroup] = dict()
            for metric in os.listdir(cgroup_path):
                metric_path = os.path.join(cgroup_path, metric)
                # if metric is readable
                if os.path.isfile(metric_path) and os.access(metric_path, os.R_OK):
                    # get file content
                    try:
                        with open(metric_path) as f:
                            lines = f.read().splitlines()
                    except IOError as e:
                        container_metrics[cgroup][metric] = str(e)
                        continue
                    if not len(lines):
                        continue
                    container_metrics[cgroup][metric] = treatMetric(cgroup, metric, lines)

            # reparse all generated metrics to add some computed fields
            for metric in container_metrics[cgroup].keys():
                # append a _count key with the length of the key elements
                if (isinstance(container_metrics[cgroup][metric], (list, dict)) and
                    metric+'_count' not in container_metrics[cgroup]):
                    container_metrics[cgroup][metric+'_count'] = len(container_metrics[cgroup][metric])

        if len(container_metrics) == 0:
            container_metrics = None
        if not cache:
            return container_metrics
        try:
            acquireLock(local_query_cache_lock_path)
            with open(local_query_cache_path, 'w') as f:
                json.dump(container_metrics, f)
        except Exception as e:
            raise e
        finally:
            releaseLock(local_query_cache_lock_path)
    # use cache
    else:
        try:
            acquireLock(local_query_cache_lock_path)
            with open(local_query_cache_path, 'r') as f:
                container_metrics = json.load(f)
        except Exception as e:
            raise e
        finally:
            releaseLock(local_query_cache_lock_path)
    return container_metrics


## RETRIEVE INFORMATIONS
if __name__ == "__main__":
    # Create parser
    parser = argparse.ArgumentParser(description='Command line utility to query containers stats from docker daemon')
    id_group = parser.add_mutually_exclusive_group(required=True)
    id_group.add_argument('--container-name', action='store', dest='container_name',
                                help='Get stats about this container name using docker api')
    id_group.add_argument('--container-id', action='store', dest='container_id',
                                help='Get stats about this container name using docker sys fs')
    parser.add_argument('--no-cache', action='store_false', dest='use_cache', default=True,
                                help='Disable caching of docker queries')

    args = parser.parse_args()

    # ensure cache directory
    if args.use_cache:
        if os.path.exists(local_query_cache_dir_path):
            if not os.path.isdir(local_query_cache_dir_path):
                os.unlink(local_query_cache_dir_path)
                os.mkdir(local_query_cache_dir_path, 0o0770)
        else:
            os.mkdir(local_query_cache_dir_path, 0o0770)

    ## OUTPUT
    if args.container_name:
        content = getContainersStatsFromApi(args.container_name, cache=args.use_cache)
    elif args.container_id:
        content = getContainersStatsFromSysFs(args.container_id, cache=args.use_cache)
    print(json.dumps(content))
sys.exit(0)
