#!/usr/bin/env python

import argparse
import docker
import hashlib
import json
import os
import re
import sys
import time

# Return dict
content=None

# python 3
if sys.version_info[0] == 3:
    string_types = str,
else:
    string_types = basestring,

# Global variables
# mapping for return values
attributes_map = {
}

# number of second to cache a query result
local_query_cache_life = 55
local_query_cache_dir_path = '/tmp/zbx_dck_csqc_{}'.format(hashlib.md5(os.getenv('USER').encode()).hexdigest())

sysfs_base_path = '/sys/fs/cgroup'
sysfs_metrics_config = {
    'cpu': {},
    'cpuacct': {},
    'memory': {},
    'blkio': {},
}

re_named_metric = re.compile('(?P<name>[a-zA-Z][a-zA-Z0-9_-]*)\s*(?P<metric>[0-9.]+)')

#os.environ['DOCKER_HOST']='{{ docker_server__clients_host }}'


def LockException(BaseException):
    pass

def acquireLock(lockfile):
    tries = 0
    # wait if lock is acquired
    while os.path.isfile(lockfile):
        tries += 1
        if tries > 14:
            raise LockException('unable to acquire lock')
        time.sleep(0.1)
    with open(lockfile, 'w') as f:
        pass
    return True

def releaseLock(lockfile):
    if os.path.isfile(lockfile):
        os.unlink(lockfile)
    return True

def getContainersStatsFromApi(container_name, cache=True):
    local_query_cache_path = os.path.join(local_query_cache_dir_path, container_name)
    local_query_cache_lock_path = local_query_cache_path+'.lock'

    # generate cache
    if (not cache or
        not os.path.isfile(local_query_cache_path) or
        time.time() - os.stat(local_query_cache_path).st_mtime > local_query_cache_life):
        try:
            client = docker.from_env()
            if hasattr(client.containers, 'get'):
                stats = client.containers.get(container_name).stats(stream=False)
            else:
                stats = client.stats(container_name, stream=False)

            if not cache:
                return stats
            acquireLock(local_query_cache_lock_path)
            with open(local_query_cache_path, 'w') as f:
                json.dump(stats, f)
        except docker.errors.NotFound:
            return None
        except Exception as e:
            raise e
        finally:
            releaseLock(local_query_cache_lock_path)
    # use cache
    else:
        try:
            acquireLock(local_query_cache_lock_path)
            with open(local_query_cache_path, 'r') as f:
                stats = json.load(f)
        except Exception as e:
            raise e
        finally:
            releaseLock(local_query_cache_lock_path)
    return stats

def getContainersStatsFromSysFs(container_id, cache=True):
    container_metrics = dict()
    for cgroup in sysfs_metrics_config:
        cgroup_path = os.path.join(sysfs_base_path, cgroup, 'docker', container_id)
        if not os.path.isdir(cgroup_path):
            continue

        # read all metrics from files
        container_metrics[cgroup] = dict()
        for metric in os.listdir(cgroup_path):
            metric_path = os.path.join(cgroup_path, metric)
            # if metric is readable
            if os.path.isfile(metric_path) and os.access(metric_path, os.R_OK):
                # get file content
                try:
                    with open(metric_path) as f:
                        lines = f.read().splitlines()
                except IOError as e:
                    container_metrics[cgroup][metric] = str(e)
                    continue
                if not len(lines):
                    continue
                # test for metric format

                # Named metrics are in format : "NAME VALUE"
                named_metric = re_named_metric.match(lines[0])
                if named_metric:
                    container_metrics[cgroup][metric] = dict()
                    for line in lines:
                        named_metric = re_named_metric.match(lines[0])
                        if named_metric:
                            container_metrics[cgroup][metric][named_metric.group('name')] = named_metric.group('metric')
                else:
                    container_metrics[cgroup][metric] = []
                    for line in lines:
                        container_metrics[cgroup][metric].append(line)

        # reparse all generated metrics to add some computed fields
        for metric in container_metrics[cgroup].keys():
            if (isinstance(container_metrics[cgroup][metric], list) and
                metric+'_count' not in container_metrics[cgroup]):
                container_metrics[cgroup][metric+'_count'] = len(container_metrics[cgroup][metric])


    if len(container_metrics) == 0:
        return None
    return container_metrics


## RETRIEVE INFORMATIONS
if __name__ == "__main__":
    # Create parser
    parser = argparse.ArgumentParser(description='Command line utility to query containers stats from docker daemon')
    id_group = parser.add_mutually_exclusive_group(required=True)
    id_group.add_argument('--container-name', action='store', dest='container_name',
                                help='Get stats about this container name using docker api')
    id_group.add_argument('--container-id', action='store', dest='container_id',
                                help='Get stats about this container name using docker sys fs')
    parser.add_argument('--no-cache', action='store_false', dest='use_cache', default=True,
                                help='Disable caching of docker queries')

    args = parser.parse_args()

    # ensure cache directory
    if args.use_cache:
        if os.path.exists(local_query_cache_dir_path):
            if not os.path.isdir(local_query_cache_dir_path):
                os.unlink(local_query_cache_dir_path)
                os.mkdir(local_query_cache_dir_path, 0o0770)
        else:
            os.mkdir(local_query_cache_dir_path, 0o0770)

    ## OUTPUT
    if args.container_name:
        content = getContainersStatsFromApi(args.container_name, cache=args.use_cache)
    elif args.container_id:
        content = getContainersStatsFromSysFs(args.container_id, cache=args.use_cache)
    print(json.dumps(content))
sys.exit(0)
